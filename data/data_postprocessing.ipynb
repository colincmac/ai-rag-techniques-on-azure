{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "if parent_dir not in sys.path:\n",
    "  sys.path.insert(0, parent_dir)\n",
    "\n",
    "load_dotenv() #import \n",
    "\n",
    "start_year = 2019\n",
    "end_year = 2024\n",
    "\n",
    "output_dir = \"sec_data\"\n",
    "load_edgar_index = False\n",
    "load_10k = False\n",
    "load_13f = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils.python_helpers import edgar_service\n",
    "import pandas as pd\n",
    "edgar = edgar_service.EdgarService(\"cmccullough\", \"cmccullough@microsoft.com\")\n",
    "if load_edgar_index:\n",
    "\n",
    "  s_and_p = pd.read_csv(\"sec_data/s&p 500.csv\")\n",
    "  ref = pd.read_csv(\"sec_data/reference_data.csv\")\n",
    "  cikfilter = s_and_p[\"CIK\"].astype(str)\n",
    "  cikfilter2 = ref[\"cik\"].astype(str)\n",
    "  edgar.parse_master_index_file(cik_filter=cikfilter, output_dir=\"sec_data\")\n",
    "  # edgar.download_filings_for_form(\"10-K\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_10k:\n",
    "  from utils.python_helpers import form_10k_extractor\n",
    "  extractor = form_10k_extractor.Form10kExtractor()\n",
    "\n",
    "  extractor.process_directory_files(\"./sec_data/reference_data.csv\", \"./sec_data/10k-raw\", \"./sec_data/10k_extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils.python_helpers.form_13f_hr_extractor import Form13F_HR_Extractor\n",
    "import pandas as pd\n",
    "download_13F_HR = False\n",
    "process_13F_HR = True\n",
    "output_dir = \"sec_data\"\n",
    "reference = pd.read_json(\"sec_data/merged_data.json\")\n",
    "\n",
    "if download_13F_HR:\n",
    "  cikfilter = reference[\"cik\"].astype(str)\n",
    "  edgar.download_filings_for_form(form=\"13F-HR\", output_dir=output_dir)\n",
    "\n",
    "if process_13F_HR:\n",
    "  extractor = Form13F_HR_Extractor(input_dir=f\"{output_dir}/13F-HR\", output_dir=output_dir)\n",
    "  extractor.process_directory_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "reference = pd.read_json(\"sec_data/merged_data.json\")\n",
    "reference = reference.set_index(\"cik\")\n",
    "display(reference)\n",
    "# Ensure 'referenceNames' column exists and is initialized as a list if it doesn't exist\n",
    "if 'referenceNames' not in reference.columns:\n",
    "    reference['referenceNames'] = [[] for _ in range(len(reference))]\n",
    "\n",
    "# with open(\"sec_data/merged_data.json\") as f:\n",
    "#   reference_json = json.load(f)\n",
    "\n",
    "sec_13f = pd.read_json(\"sec_data/13f_hr_data.json\")\n",
    "# # sec_13f = sec_13f.set_index(\"cik\")\n",
    "\n",
    "# for record in reference_json:\n",
    "#     if record['securities']:\n",
    "#         cusip_formatted = record['securities'][0]['cusip'].rjust(9, '0')\n",
    "#         record['cusip6'] = cusip_formatted[:6]\n",
    "#         # for security in record['securities']:\n",
    "#         #     sec_13f.loc[sec_13f[\"cusip6\"] == security['cusip6'], \"holdings\"] += security['holdings']\n",
    "#     else:\n",
    "#         record['cusip6'] = \"\"\n",
    "# json.dump(reference_json, open(\"sec_data/merged_data.json\", \"w\"), indent=2)\n",
    "# for each record in reference, find the corresponding record in sec_13f\n",
    "# if it exists, add the holdings to the reference data\n",
    "def add_security_if_not_exists(cik, new_security):\n",
    "    if cik in reference.index:\n",
    "        securities = reference.at[cik, 'securities']\n",
    "        cusips = [security['cusip'] for security in securities]\n",
    "        if new_security['cusip'] not in cusips:\n",
    "            securities.append(new_security)\n",
    "            reference.at[cik, 'securities'] = securities\n",
    "    else:\n",
    "        print(f\"CIK {cik} not found in reference data.\")\n",
    "\n",
    "\n",
    "with open(\"sec_data/cik-lookup-data.txt\") as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "cik_lookup = {}\n",
    "for line in lines:\n",
    "  parts = line.rsplit(':', 2)\n",
    "  # print(f\"{parts[0]}, {parts[1]}\")\n",
    "  if int(parts[1]) in reference.index:    \n",
    "    cik_lookup[int(parts[1])] = parts[0]\n",
    "\n",
    "display(cik_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "reference = pd.read_json(\"sec_data/reference_entities.json\")\n",
    "form13d = pd.read_json(\"sec_data/13d-data.json\")\n",
    "\n",
    "reference.set_index('cik', inplace=True)\n",
    "\n",
    "def add_security_if_not_exists(cik, new_security):\n",
    "    if cik in reference.index:\n",
    "        securities = reference.at[cik, 'securities']\n",
    "        cusips = [security['cusip'] for security in securities]\n",
    "        if new_security['cusip'] not in cusips:\n",
    "            securities.append(new_security)\n",
    "            reference.at[cik, 'securities'] = securities\n",
    "    else:\n",
    "        print(f\"CIK {cik} not found in reference data.\")\n",
    "found = []\n",
    "notfound = []\n",
    "for index, row in list(reference.iterrows()):\n",
    "  lookup_value = form13d.loc[form13d[\"CIK\"] == index]['CUSIP'].unique()\n",
    "  if lookup_value.size > 0:\n",
    "      found.append(index)\n",
    "      for cusip in lookup_value:\n",
    "        security = {\"cusip\": cusip, \"isin\": \"\"}\n",
    "        add_security_if_not_exists(index, security)\n",
    "  else:\n",
    "      notfound.append(index)\n",
    "  # names.append(row['companyName'].upper().rstrip(\".\"))\n",
    "  print(lookup_value)\n",
    "print(len(found))\n",
    "print(len(notfound))\n",
    "\n",
    "reference.reset_index(inplace=True)\n",
    "reference.to_json(\"sec_data/reference_entities2.json\", orient='records', indent=4)\n",
    "# if 'referenceNames' not in reference.columns:\n",
    "#     reference['referenceNames'] = [[] for _ in range(len(reference))]\n",
    "\n",
    "\n",
    "# with open(\"sec_data/cik-lookup-data.txt\") as f:\n",
    "#   lines = f.readlines()\n",
    "# print(1067983 in reference.index)\n",
    "# cik_lookup = defaultdict(list)\n",
    "# for line in lines:\n",
    "#   parts = line.rsplit(':', 2)\n",
    "#   if int(parts[1]) in reference.index: \n",
    "#     print(f\"{parts[1]}, {parts[0]}\")\n",
    "#     cik_lookup[int(parts[1])].append(parts[0].upper().rstrip(\".\"))\n",
    "\n",
    "# for index, row in list(reference.iterrows()):\n",
    "#   names = [value.upper().rstrip(\".\") for value in reference.at[index, 'referenceNames']]\n",
    "#   lookup_value = cik_lookup.get(int(index))\n",
    "#   names.append(row['companyName'].upper().rstrip(\".\"))\n",
    "\n",
    "#   if lookup_value:\n",
    "#     print(f\"Adding {lookup_value} to {row['companyName']}\")\n",
    "#     names.extend(lookup_value)\n",
    "#   print(len(names))\n",
    "#   reference.at[index, 'referenceNames'] = list(set(names))\n",
    "\n",
    "# reference.reset_index(inplace=True)\n",
    "# # display(reference)\n",
    "# reference.to_json(\"sec_data/reference_entities2.json\", orient='records', indent=4)\n",
    "\n",
    "# for index, row in list(reference.iterrows())[0:10]:\n",
    "#   print(row['referenceNames'])\n",
    "# for row in list(reference.itertuples())[0:10]:\n",
    "#   print(row['companyName'])\n",
    "# for line in lines:\n",
    "\n",
    "#   line = line.strip()\n",
    "#   # [name, cik, empty] = line.split(':')\n",
    "#   # [empty, name, cik, empty] = line.split(':')\n",
    "\n",
    "#   parts = line.rsplit(':', 2)\n",
    "#   name = parts[0]\n",
    "#   cik = int(parts[1])\n",
    "#   if not cik in reference.index:\n",
    "#      print(\"skipping cik: \", cik)\n",
    "#      continue\n",
    "#   print(f\"Processing {name} ({cik})\")\n",
    "#   if not isinstance(reference.at[cik, 'referenceNames'], list):\n",
    "#     reference.at[cik, 'referenceNames'] = []\n",
    "#   current_name = reference.at[cik, 'companyName'].upper()\n",
    "#   names = [name.upper(), reference.at[cik, 'companyName'].upper()] + reference.at[cik, 'referenceNames']\n",
    "#   reference.at[cik, 'referenceNames'] = list(set(names))\n",
    "\n",
    "#   cusip6 = reference.at[cik, 'cusip6']\n",
    "#   if cusip6:\n",
    "#      sec_13_entity_cusips = sec_13f.loc[sec_13f[\"cusip6\"] == cusip6]['cusip'].unique()\n",
    "#     #  ref_entity_cusips = ref_entity['securities']\n",
    "#     #  print(ref_entity_cusips)\n",
    "#     #  sec_13_entity_cusips = sec_13f.loc[sec_13f[\"cusip6\"] == ref_entity['cusip6']]['cusip'].unique()\n",
    "#     #  add_security_if_not_exists(cik, ref_entity)\n",
    "#     #  print(sec_13_entity_cusips)\n",
    "#   else:  \n",
    "#      sec_13_entity_cusips = sec_13f.loc[sec_13f[\"companyName\"] == name]['cusip'].unique()\n",
    "\n",
    "#   for cusip in sec_13_entity_cusips:\n",
    "#     security = {\"cusip\": cusip, \"isin\": \"\"}\n",
    "#     add_security_if_not_exists(cik, security)\n",
    "#   # display(reference.loc[cik, 'securities'])\n",
    "#   # display(reference.loc[cik, 'referenceNames'])\n",
    "# reference.reset_index()\n",
    "# reference.to_json(\"sec_data/reference_entities.json\", orient='records', indent=4)\n",
    "#   cusips = sec_13f.loc[sec_13f[\"companyName\"] == name].value_counts(\"cusip\")\n",
    "#   print(cusips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from utils.python_helpers import form_13d_extractor\n",
    "\n",
    "form_13d_extractor.process_directory(\"sec_data/13D-raw\", \"sec_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils.python_helpers import finance_reports_service\n",
    "import pandas as pd\n",
    "\n",
    "company_cik_ticker_ref = pd.read_json(\"sec_data/sec_company_tickers.json\")\n",
    "\n",
    "finance_reports_service = finance_reports_service.FinanceReportsService(\"cmcullough\", \"cmccullough@microsoft.com\", company_cik_ticker_ref, \"sec_data\")\n",
    "\n",
    "msftInfo = finance_reports_service.get_current_market_data(789019)\n",
    "msftInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils.python_helpers import financial_modeling_service\n",
    "\n",
    "\n",
    "financial_modeling_service.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([355379.93046613, 244443.25173427, 168136.96609162, 115650.72533574,\n",
       "        79548.77848452])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils.python_helpers.fin_models import subscription_based\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "years = np.array([2018, 2019, 2020, 2021, 2022])\n",
    "# users = np.array([500000, 750000, 1100000, 1600000, 2300000])\n",
    "users = np.array([2300000,1600000,1100000,750000,500000])\n",
    "\n",
    "future_years = np.arange(2023, 2028)\n",
    "scenario = subscription_based.ScenarioAssumptions()\n",
    "\n",
    "scenario.forecast_exp_growth(years=years, metric=users, forecast_years=future_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils.python_helpers.sec_nport_processor import process_nport\n",
    "process_nport(\"sec_data/vti_nport.xml\", \"sec_data/sec_company_tickers.json\", \"sec_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "output_file = 'sec_data/merged_data.json'\n",
    "def merge_data():\n",
    "    # Read JSON file\n",
    "    with open(output_file, mode='r') as file:\n",
    "        json_data = json.load(file)\n",
    "    # Create a dictionary to store combined data\n",
    "    combined_data = {}\n",
    "\n",
    "    for entry in sec_ref_data:\n",
    "        cik = entry[\"cik\"]\n",
    "        if cik in combined_data:\n",
    "            continue\n",
    "        else:\n",
    "            tickers = [sec_entry[\"ticker\"] for sec_entry in grouped_sec_data[cik]]\n",
    "            securities = [{\"isin\": json_entry[\"isin\"], \"cusip\": json_entry[\"cusip\"]} for json_entry in grouped_json_data[cik]]\n",
    "            lei_list = [json_entry[\"lei\"] for json_entry in grouped_json_data[cik] if json_entry[\"lei\"]]\n",
    "            print((cik, lei_list))\n",
    "            val = {\n",
    "                \"cik\": cik,\n",
    "                \"lei\": lei_list[0] if lei_list else \"\",\n",
    "                \"companyName\": entry[\"companyName\"],\n",
    "                \"tickers\": tickers,\n",
    "                \"securities\": securities\n",
    "            }\n",
    "            if cik in csv_data:\n",
    "                val[\"cusips\"] = csv_data[cik][\"cusips\"].split(\",\")\n",
    "\n",
    "            combined_data[cik] = val\n",
    "    # Convert dictionary back to JSON array\n",
    "    output_data = list(combined_data.values())\n",
    "\n",
    "    # Write output JSON file\n",
    "    with open(output_file, mode='w') as file:\n",
    "        json.dump(output_data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "json_files = glob(\"sec_data\\\\basic_financials\\\\*.json\")\n",
    "merged = 'sec_data/merged_data.json'\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, \"r+\") as f:\n",
    "        data = json.load(f)\n",
    "        if 'cusips' in data:\n",
    "            value = data['cusips']\n",
    "            if isinstance(value, list):\n",
    "                continue\n",
    "            elif isinstance(value, float):\n",
    "                value = []\n",
    "            else:\n",
    "                value = value.split(\",\")\n",
    "            data[\"cusips\"] = value\n",
    "        else:\n",
    "            data[\"cusips\"] = []\n",
    "        f.seek(0)\n",
    "        json.dump(data, f, indent=4)\n",
    "        f.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "sector_data = pd.read_csv(\"sector_data/combined2.csv\")\n",
    "reference_data = pd.read_json(\"sec_data/company_data.json\", orient='records')\n",
    "reference_data.set_index(\"cik\", inplace=True)\n",
    "found_ticker = []\n",
    "found_name = []\n",
    "notfound = []\n",
    "\n",
    "for index, row in list(reference_data.iterrows()):\n",
    "  if row[\"sector\"] != None:\n",
    "    continue\n",
    "  sector_info = None\n",
    "  if len(row[\"tickers\"]) > 0:\n",
    "    sector_info = sector_data.loc[sector_data[\"Symbol\"].isin(row[\"tickers\"])]\n",
    "    if not sector_info.empty:\n",
    "      found_ticker.append((index, sector_info[\"Symbol\"], sector_info[\"Sector\"]))\n",
    "      reference_data.at[index, \"sector\"] = sector_info.iloc[0][\"Sector\"]\n",
    "      reference_data.at[index, \"industry\"] = sector_info.iloc[0][\"Industry\"]\n",
    "      reference_data.at[index, \"subindustry\"] = sector_info.iloc[0][\"Sub-Industry\"]\n",
    "      continue\n",
    "  if sector_info.empty or sector_info == None:\n",
    "    sector_info = sector_data.loc[sector_data[\"Company Name\"].str.upper().isin(row[\"referenceNames\"])]\n",
    "    if not sector_info.empty:\n",
    "      reference_data.at[index, \"sector\"] = sector_info.iloc[0][\"Sector\"]\n",
    "      reference_data.at[index, \"industry\"] = sector_info.iloc[0][\"Industry\"]\n",
    "      reference_data.at[index, \"subindustry\"] = sector_info.iloc[0][\"Sub-Industry\"]\n",
    "      tickers = reference_data.at[index, \"tickers\"]\n",
    "      tickers.extend(sector_info[\"Symbol\"])\n",
    "      reference_data.at[index, \"tickers\"] = set(tickers)\n",
    "      found_name.append((index, sector_info[\"Symbol\"], sector_info[\"Sector\"]))\n",
    "      continue\n",
    "  notfound.append(index)\n",
    "reference_data.reset_index(inplace=True)\n",
    "reference_data.to_json(\"sec_data/reference_entities3.json\", orient='records', indent=4)\n",
    "print(len(found_ticker))\n",
    "print(len(found_name))\n",
    "print(len(notfound))\n",
    "print(notfound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
